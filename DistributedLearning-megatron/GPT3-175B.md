# GPT3-175B

## 模型架构

### Docoder-Only
```
Embedding层 + 位置编码层 + Attention层 + Feed Forward层
```

### 参数量

`Embedding层`：$n_{vocab} * d_{model}$

`位置编码层`：$n_{ctx} * d_{model}$.

`Attention层`：一般来说，$d_{model} = n_{head} * d_{head}$，存在 $Q,K,V,O$. 每一个是 $((n_{head} * d_{head}),d_{model})$，也就是 $d_{model} * d_{model}$. 总共是4个，那么算出来就是$4 * d_{model}^2$

`Feed Forward层`：存在`升维`与`降维`，shape分别是$(d_{model},d_{ff})$与$(d_{ff},d_{model})$，且一般来说$d_{ff} = 4 * d_{model}$，两者均存在`bias`，那么总共的参数是$8 * d_{model}^2 + 5 * d_{model}$

假设，层数是$n_{layer}$，那么总共的参数量就是$n_{vocab}*d_{model}$ + $n_{ctx}*d_{model}$ + $12*d_{model}^2$ + $5*d_{model}$

值得注意的是，llama架构中的 $FFN$ 层存在3部分，gate、up、down，三者拼接长度是 $8*d_{model}$，则每一份就是 $8/3 * d_{model}$